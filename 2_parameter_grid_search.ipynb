{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4914ad1e-766c-42de-a921-ab48aed369c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8e16fa-bb7a-4827-acb4-5c516203c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d5c3cb4-5baf-404a-89b7-ce1c4552f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.1.2+cu118\n",
      "cuda available: True\n",
      "devices count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "print(f\"cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"devices count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2559ac-5aea-4f76-bd10-7f08794c76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed for MLP\n",
    "def balance_dataset(X, y): \n",
    "    value_counts = []\n",
    "    for i in range(5):\n",
    "        value_counts.append(y.count(i))\n",
    "    index = 0\n",
    "    added = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    add_to_y = {1: [], 2: [], 3: [], 4: []}\n",
    "    add_to_X = {1: [], 2: [], 3: [], 4: []}\n",
    "    while sum(added.values()) + sum(value_counts) < 5 * value_counts[0]:\n",
    "        if index >= len(y):\n",
    "            index = 0\n",
    "        if y[index] != 0 and added[y[index]] + value_counts[y[index]] < value_counts[0]:\n",
    "            add_to_y[y[index]].append(y[index])\n",
    "            add_to_X[y[index]].append(X[index])\n",
    "            added[y[index]] += 1\n",
    "        index += 1\n",
    "    new_X = X.copy()\n",
    "    new_y = y.copy()\n",
    "    index = 0\n",
    "    while index < sum(added.values()):\n",
    "        for i in range(1, 5):\n",
    "            if len(add_to_y[i]) > 0:\n",
    "                new_y.append(add_to_y[i].pop(0))\n",
    "                new_X.append(add_to_X[i].pop(0))\n",
    "                index += 1\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63ebfad-0336-43e6-ba71-fdf81c3e9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedJsonLInputFile(model, path):\n",
    "    input = pd.read_json(path, lines=True).set_index('id')\n",
    "    e = pd.DataFrame(index=input.index, columns=['Embedding'])\n",
    "    for idx in tqdm(input.index):\n",
    "        sentence = input.loc[idx]['text']\n",
    "        e.loc[idx]['Embedding'] = model.encode(sentence)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856cd7d2-2205-4522-96b3-1ab06c9188e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_st1(predictions):\n",
    "    #list with columns [\"id\": ..., \"A001\": \"1.0, \"A002\": 0.0, \"A003\": ..., ..., \"A012\": ...] \n",
    "    QUANT_TO_QUAL={0: 0, 1: 1, 2: 1, 3: 1, 4: 1}\n",
    "    NUMBER_TO_LABEL={0: \"0-Kein\", 1: \"1-Gering\", 2: \"2-Vorhanden\", 3: \"3-Stark\", 4: \"4-Extrem\"}\n",
    "    \n",
    "    #predictions_quant: pd.DataFrame = predictions.applymap(lambda x: LABEL_VALS_QUANT[x] if not pd.isna(x) else x)\n",
    "    predictions_qual: pd.DataFrame = predictions.applymap(lambda x: QUANT_TO_QUAL[x] if not pd.isna(x) else x)\n",
    "    \n",
    "    output = pd.DataFrame(index=[\"id\"])\n",
    "    output.index = predictions.index\n",
    "    \n",
    "    #create expected columns\n",
    "    output[\"bin_maj\"] = predictions_qual.mode(axis='columns')[0]\n",
    "    output[\"bin_one\"] = predictions_qual.apply(lambda x: (x == 1).any(), axis='columns')\n",
    "    output[\"bin_all\"] = predictions_qual.apply(lambda x: not (x == 0).any(), axis='columns')\n",
    "    output[\"multi_maj\"] = predictions.mode(axis='columns')[0].apply(lambda x: NUMBER_TO_LABEL[x])\n",
    "    output[\"disagree_bin\"] = output.apply(lambda x: x[\"bin_one\"] and not x[\"bin_all\"], axis='columns')\n",
    "    \n",
    "    #convert False/True to 0/1\n",
    "    output['bin_maj'] = output['bin_maj'].apply(lambda x: 1 if x else 0)\n",
    "    output['bin_one'] = output['bin_one'].apply(lambda x: 1 if x else 0)\n",
    "    output['bin_all'] = output['bin_all'].apply(lambda x: 1 if x else 0)\n",
    "    output['disagree_bin'] = output['disagree_bin'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e04453c-a67a-4bad-ba49-02d4efc7fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedders = [\n",
    "    {'model': 'intfloat/multilingual-e5-base', 'model_ident': 'me5-base'},\n",
    "    {'model': 'intfloat/multilingual-e5-large', 'model_ident': 'me5-large'},\n",
    "    {'model': 'deutsche-telekom/gbert-large-paraphrase-cosine', 'model_ident': 't-gbert-lpc'}\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'model': SVC(kernel='rbf', C=5, class_weight=\"balanced\", random_state=SEED), \n",
    "        'model_ident': 'svc', \n",
    "        'prerun': None,\n",
    "        'param_grid': {'C': [1, 2, 5, 8, 13, 21, 35, 56, 91]}\n",
    "    }, {\n",
    "        'model': RandomForestClassifier(class_weight=\"balanced\", criterion=\"gini\", random_state=SEED), \n",
    "        'model_ident': 'rfc', \n",
    "        'prerun': None,\n",
    "        'param_grid': {'max_depth': [1, 2, 5, 8, 13, 21, 35, 56, 91], 'n_estimators': [10, 20, 50, 80, 130, 210, 350, 560]}\n",
    "    }, {\n",
    "        'model': MLPClassifier(early_stopping=True, n_iter_no_change=10, random_state=SEED, max_iter=1000), \n",
    "        'model_ident': 'mlp', \n",
    "        'prerun': balance_dataset,\n",
    "        'param_grid': {'hidden_layer_sizes': [(64,), (128,), (256,), (512,), (1024,), (1536,), (2048,)]}\n",
    "    }\n",
    "]\n",
    "\n",
    "annotators = [\"A001\", \"A002\", \"A003\", \"A004\", \"A005\", \"A007\", \"A008\", \"A009\", \"A010\", \"A012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26798741-0cc6-4148-96eb-2661d2886f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model: me5-base\n",
      "Classification Model: svc\n",
      "Annotator: A001\n"
     ]
    }
   ],
   "source": [
    "annotator_model_scores = {}\n",
    "models = {}\n",
    "\n",
    "for embedder in embedders:\n",
    "    print(f\"Embedding Model: {embedder['model_ident']}\")\n",
    "\n",
    "    annotator_model_scores[embedder[\"model_ident\"]] = {}\n",
    "    models[embedder[\"model_ident\"]] = {}\n",
    "    \n",
    "    embeddings = 0\n",
    "    embedding_model = SentenceTransformer(embedder['model'])\n",
    "    \n",
    "    if os.path.isfile(f\"created_data/embeddings/{embedder['model_ident']}.pkl\"):\n",
    "        embeddings = pd.read_pickle(f\"created_data/embeddings/{embedder['model_ident']}.pkl\")\n",
    "    else:\n",
    "        embeddings = embedJsonLInputFile(embedding_model, 'created_data/training_data/X_all.jsonl')\n",
    "        embeddings.to_pickle(f\"created_data/embeddings/{embedder['model_ident']}.pkl\")\n",
    "\n",
    "    if not os.path.isfile(f\"created_data/embeddings/{embedder['model_ident']}_test.pkl\"):\n",
    "        embeddings_test = embedJsonLInputFile(embedding_model, 'created_data/training_data/X_test.jsonl')\n",
    "        embeddings_test.to_pickle(f\"created_data/embeddings/{embedder['model_ident']}_test.pkl\")\n",
    "\n",
    "    embedding_dim = len(embeddings.iloc[0])\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        print(f\"Classification Model: {classifier['model_ident']}\")\n",
    "\n",
    "        annotator_model_scores[embedder['model_ident']][classifier['model_ident']] = {}\n",
    "        models[embedder['model_ident']][classifier['model_ident']] = {}\n",
    "\n",
    "        \n",
    "        for annotator in annotators:\n",
    "            score = 0\n",
    "            print(f\"Annotator: {annotator}\")\n",
    "            \n",
    "            y_train = pd.read_json(f\"created_data/training_data/y_train_{annotator}.jsonl\", lines=True).set_index('id')\n",
    "            X_train = embeddings.loc[y_train.index]\n",
    "            \n",
    "            y_val = pd.read_json(f\"created_data/training_data/y_val_{annotator}.jsonl\", lines=True).set_index('id')\n",
    "            X_val = embeddings.loc[y_val.index]\n",
    "\n",
    "            models[embedder['model_ident']][classifier['model_ident']][annotator] = clone(classifier['model'])\n",
    "            classifier_model = models[embedder['model_ident']][classifier['model_ident']][annotator]\n",
    "\n",
    "            cached = False\n",
    "            with open(f\"created_data/cache.txt\", 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if line.strip() == f\"{embedder['model_ident']}_{classifier['model_ident']}_{annotator}\":\n",
    "                        cached = True\n",
    "                        break\n",
    "            f.close()\n",
    "            if not cached:\n",
    "                clf = GridSearchCV(classifier_model, classifier['param_grid'], n_jobs=100, cv=5, refit=True)\n",
    "                \n",
    "                X_train_list = X_train['Embedding'].to_list()\n",
    "                y_train_list = y_train[annotator].to_list()\n",
    "    \n",
    "                if classifier[\"prerun\"] is not None:\n",
    "                    X_train_list, y_train_list = classifier[\"prerun\"](X_train_list, y_train_list)\n",
    "    \n",
    "                X_val_list = X_val['Embedding'].to_list()\n",
    "                y_val_list = y_val[annotator].to_list()\n",
    "    \n",
    "                clf.fit(X_train_list, y_train_list)    \n",
    "                print(clf.best_params_)\n",
    "                score = clf.score(X_val_list, y_val_list)\n",
    "\n",
    "                with open(f\"models/{embedder['model_ident']}_{classifier['model_ident']}_{annotator}.pkl\", 'w+b') as f:\n",
    "                    pickle.dump(clf.best_estimator_, f)\n",
    "                f.close()\n",
    "                with open(f\"created_data/cache.txt\", 'a') as f:\n",
    "                    f.write(f\"{embedder['model_ident']}-{classifier['model_ident']}-{annotator}\\n\")\n",
    "                f.close()    \n",
    "            else:\n",
    "                with open(f\"models/{embedder['model_ident']}_{classifier['model_ident']}_{annotator}.pkl\", 'rb') as f:\n",
    "                    cached_model = pickle.load(f)\n",
    "                f.close()    \n",
    "                print(f\"Cached: {cached_model}\")\n",
    "                score = cached_model.score(X_val_list, y_val_list)\n",
    "            \n",
    "            print(score)\n",
    "            annotator_model_scores[embedder['model_ident']][classifier['model_ident']][annotator] = score     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp-venv6",
   "language": "python",
   "name": "bp-venv6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
